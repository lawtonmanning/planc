\documentclass[conference,compsoc]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.

\usepackage{cite,doi}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithm,algpseudocode}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{cleveref}
\usepackage{tikz}
\usepackage{mathtools}
\usepackage{ourmacros}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\newcommand{\LM}[1]{\textcolor{blue}{\textbf{Lawton}: #1}}    
\newcommand{\GB}[1]{\textcolor{red}{\textbf{Grey}: #1}}
\newcommand{\RK}[1]{\textcolor{blue}{\textbf{Ramki}: #1}}


\begin{document}

\title{Parallel Hierarchical Clustering using \\ Rank-Two Nonnegative Matrix Factorization
\thanks{This material is based upon work supported by the National Science Foundation under Grant No. OAC-1642385 and OAC-1642410.
This manuscript has been co-authored by UT-Battelle, LLC under Contract No. DE-AC05-00OR22725 with the U.S. Department of
Energy. This project was partially funded by the Laboratory Director's Research and Development fund. This research used resources
of the Oak Ridge Leadership Computing Facility at the Oak Ridge National Laboratory, which is supported by the Office of Science of
the U.S. Department of Energy.}
}

\author{\IEEEauthorblockN{Lawton Manning and Grey Ballard}
\IEEEauthorblockA{Wake Forest University \\
Winston-Salem, NC, USA \\
\{mannlg15,ballard\}@wfu.edu}
\and
\IEEEauthorblockN{Ramakrishnan Kannan}
\IEEEauthorblockA{Oak Ridge National Laboratory \\
Oak Ridge, TN, USA \\
kannanr@ornl.gov}
\and
\IEEEauthorblockN{Haesun Park}
\IEEEauthorblockA{Georgia Institute of Technology \\
Atlanta, GA, USA \\
hpark@cc.gatech.edu}
}

\maketitle

\begin{abstract}
Nonnegative Matrix Factorization (NMF) is an effective tool for clustering nonnegative data, either for computing a flat partitioning of a dataset or for determining a hierarchy of similarity.
In this paper, we propose a parallel algorithm for hierarchical clustering that uses a divide-and-conquer approach based on rank-two NMF to split a data set into two cohesive parts.
Not only does this approach uncover more structure in the data than a flat NMF clustering, but also rank-two NMF can be computed more quickly than for general ranks, providing comparable overall time to solution.
Furthermore, the rank-two NMF algorithm can be parallelized efficiently using a computational kernel similar to matrix-vector multiplication.
Our data distribution and parallelization strategies are designed to maintain computational load balance throughout the data-dependent hierarchy of computation while limiting interprocess communication, allowing the algorithm to scale to large dense and sparse data sets.
We demonstrate the scalability of our parallel algorithm in terms of data size and number of processors, and apply the hierarchical clustering approach to large (dense) hyperspectral imaging and (sparse) text document data.
\end{abstract}

%\begin{IEEEkeywords}
%component, formatting, style, styling, insert
%\end{IEEEkeywords}

\section{Introduction}

\GB{ask Haesun to write intro paragraph motivating hierarchical clustering but noting the limitations of time and data size}

\section{Preliminaries}

\begin{itemize}
	\item NMF, BCD, NNLS
	\item PLANC
\end{itemize}

\section{Algorithm}

\subsection{Sequential Algorithm}

\subsubsection{Rank-2 NMF}

\begin{itemize}
	\item subroutine for splitting node is rank-2 NMF
	\item using BCD, subproblem is set of independent NNLS problems with variable vector of length 2
	\item 4 possibilities
	\begin{itemize}
		\item all-zero option never optimum if input is nonnegative
		\item if all-pos option is optimum, it is solution to unconstrained problem
		\item can choose between one-zero options using line 6 of algorithm
		\item describe \cref{alg:r2nmf}
	\end{itemize}
\end{itemize}

Using the 2-block BCD approach for a rank-2 NMF yields NNLS subproblems of the form $\displaystyle \min_{\M[\bar]{H} \geq \M{0}} \|\M{W}\M[\bar]{H}^\Tra-\M{A}\|$ and $\min_{\M[\bar]{W} \geq \M{0}} \|\M{H}\M[\bar]{W}^\Tra-\M{A}^\Tra\|$.
In each case, the columns of the transposed variable matrix can be computed independently.
Considering the $i$th row of $\M[\bar]{H}$, for example, the NNLS problem to solve is
\begin{align*}
\begin{multlined}
\min_{\ME[\bar]{H}{i,1},\ME[\bar]{H}{i,2} \geq 0} \left\|\begin{bmatrix} \V{w}_1 & \V{w}_2 \end{bmatrix} \begin{bmatrix}\ME[\bar]{H}{i,1} \\ \ME[\bar]{H}{i,2} \end{bmatrix} - \V{A}_i \right\| \\ = \min_{\ME[\bar]{H}{i,1},\ME[\bar]{H}{i,2} \geq 0} \left\|\ME[\bar]{H}{i,1} \V{w}_1 + \ME[\bar]{H}{i,2} \V{w}_2 - \V{a}_i \right\|
\end{multlined}
\end{align*}
where $\V{w}_1$ and $\V{W}_2$ are the two columns of $\M{W}$ and $\V{a}_i$ is the $i$ column of $\M{A}$.
We note that there are four possibilities of solutions, as each of the two scalar variables may be positive or zero.

As shown by Kuang and Park \cite{KP13}, determining which of the four possible solutions is feasible and optimal can be done efficiently by exploiting the following properties:
\begin{itemize}
	\item if the solution to the unconstrained least squares problem admits two positive values, it is the optimal solution to the nonnegatively constrained problem,
	\item if $\M{W}$ and $\M{A}$ are both nonnegative, then the candidate solution with two zero values is never (uniquely) optimal and can be discarded, and
	\item if the solution to the unconstrained problem does not admit two positive values, the better of the two remaining solutions can be determined by comparing $\V{a}_{j}^\Tra\V{w}_1 / \|\V{w}_1\|$ and $\V{a}_{j}^\Tra\V{w}_2 / \|\V{w}_2\|$.
\end{itemize}
If the unconstrained problem is solved via the normal equations, then the temporary matrices computed for the normal equations ($\M{W}^\Tra\M{W}$ and $\M{A}^\Tra\M{W}$) can be re-used to determine the better of the two single-positive-variable solutions.

\Cref{alg:r2nmf} implements this strategy for all rows of $\M{H}$ simultaneously.
It takes as input the matrices $\M{C}=\M{A}^\Tra\M{W}$ and $\M{G}=\M{W}^\Tra\M{W}$, first solves the normal equations for the unconstrained problem, and then determines between the two alternate possibilities as necessary.
\GB{To avoid two passes through the data, we could combine the solve and the update if necessary.  It might also help to pre-compute the inverse of $\M{G}$...  Also Lawton, please check the details of the pseudocode!}

\begin{algorithm}
\caption{Rank-2 Nonnegative Least Squares Solve \cite{KP13}}
\label{alg:r2nmf}
\begin{algorithmic}[1]
	\Require{$\M{C}$ is $n\times 2$ and $\M{G}$ is $2\times 2$ and s.p.d.}
	\Function{$\M{H} =$ Rank2-NLS-Solve}{$\M{C},\M{G}$}
		\State $\M{H} = \M{C}\M{G}^{-1}$ \hfill \Comment{Solve unconstrained system}
		\For{$i=1$ to $n$}
			\If{$\ME{H}{i1} < 0$ or $\ME{H}{i2} < 0$}
				\State \Comment{Choose between single-variable solutions}
				\If{$\ME{C}{i1} / \sqrt{\ME{G}{11}}  < \ME{C}{i2} / \sqrt{\ME{G}{22}} $}
					\State $\ME{H}{i1} = 0$
					\State $\ME{H}{i2} = \ME{C}{i2} / \ME{G}{22}$
				\Else
					\State $\ME{H}{i1} = \ME{C}{i1} / \ME{G}{11}$
					\State $\ME{H}{i2} = 0$
				\EndIf
			\EndIf
		\EndFor
	\EndFunction
	\Ensure{$\displaystyle \M{H} = \argmin_{\M[\bar]{H} \geq \M{0}} \|\M{A}-\M{W}\M[\bar]{H}^\Tra\|$ is $n\times 2$ with $\M{C}=\M{A}^\Tra \M{W}$ and $\M{G}=\M{W}^\Tra\M{W}$}
\end{algorithmic}
\end{algorithm}

\subsubsection{Hierarchical Clustering}

\begin{algorithm}
\caption{Hierarchical Clustering \cite{KP13}}
\label{alg:hiernmf2}
\begin{algorithmic}[1]
	\Require{$\M{A}$ is $m\times n$, $k$ is target number of leaves}
	\Function{${\cal T} =$ Hier-R2-NMF}{$\M{A}$}
		\State ${\cal R} = \text{node}(\M{A})$ \hfill \Comment{create root node}
		\State \textsc{Split}$({\cal R})$
		\State inject$({\cal Q},{\cal R}.\text{left})$ \hfill \Comment{create priority queue}
		\State inject$({\cal Q},{\cal R}.\text{right})$ \hfill \Comment{of frontier nodes}
		\While{size$({\cal Q}) < k$}
			\State ${\cal N} = \text{eject}({\cal Q})$ \hfill \Comment{frontier node with max score}
			\State \textsc{Split}$({\cal N}.\text{left})$ \hfill \Comment{split left child}
			\State inject$({\cal Q},{\cal N}.\text{left})$ \hfill \Comment{and add to $\cal Q$}
			\State \textsc{Split}$({\cal N}.\text{right})$ \hfill \Comment{split right child}
			\State inject$({\cal Q},{\cal N}.\text{right})$ \hfill \Comment{and add to $\cal Q$}
		\EndWhile
	\EndFunction
	\Ensure{$\cal{T}$ is binary tree rooted at $\cal R$ with $k$ frontier nodes, each node has subset of cols of $\M{A}$ and feature vector $\V{w}$}
\end{algorithmic}
\end{algorithm}

\begin{figure}
\input{fig/tree}
\caption{Hierarchy node classification}
\label{fig:tree}
\end{figure}

\begin{algorithm}
\caption{Node Splitting via Rank-Two NMF}
\label{alg:split}
\begin{algorithmic}[1]
	\Require{$\cal N$ has a subset of columns given by field $\M{A}$}
	\Function{Split}{$\cal N$}
		\State $[\M{W},\M{H}] = \textsc{Rank2-NMF}({\cal N}.\M{A})$ \hfill \Comment{split $\cal N$}
		\State partition ${\cal N}.\M{A}$ into $\M{A}_1$ and $\M{A}_2$ using $\M{H}$
		\State ${\cal N}.\text{left} = \text{node}(\M{A}_1,\V{W}_1)$ \hfill \Comment{create left child}
		\State ${\cal N}.\text{right} = \text{node}(\M{A}_2,\V{W}_2)$ \hfill \Comment{create right child}
		\State ${\cal N}.\text{score} = \sigma_1(\M{A}_1) + \sigma_1(\M{A}_2) - \sigma_1({\cal N}.\M{A})$
	\EndFunction
	\Ensure{$\cal{N}$ has two children and a score}
\end{algorithmic}
\end{algorithm}

\subsection{Parallelization}

\subsubsection{Algorithm}

\begin{algorithm}
\caption{Parallel Rank-2 NMF}
\label{alg:parrank2nmf}
\begin{algorithmic}[1]
	\Require{$\M{A}$ is $m\times n$ and row-distributed across processors so that $\M[\hat]{A}$ is local $(m/p)\times n$ submatrix}
	\Function{$[\M{W},\M{H}] =$ Parallel-Rank2-NMF}{$\M{A}$}
		\State Initialize local $\M[\hat]{W}$ randomly
		\While{not converged}
			\State \Comment{Compute $\M{H}$}
			\State $\M[\hat]{G}_W = \M[\hat]{W}^\Tra \M[\hat]{W}$
			\State $\M{G}_W = \textsc{All-Reduce}(\M[\hat]{G})$
			\State $\M[\hat]{B} = \M[\hat]{W}^\Tra \M[\hat]{A}$ %\hfill \Comment{local matrix multiplication}
			\State $\M[\hat]{C} = \textsc{Reduce-Scatter}(\M[\hat]{B})$
			\State $\M[\hat]{H} = \textsc{Rank2-NLS-Solve}(\M[\hat]{C},\M{G}_W)$
			\State \Comment{Compute $\M{W}$}
			\State $\M[\hat]{G}_H = \M[\hat]{H}^\Tra \M[\hat]{H}$
			\State $\M{G}_H = \textsc{All-Reduce}(\M[\hat]{G}_H)$
			\State $\M{H} = \textsc{All-Gather}(\M[\hat]{H})$
			\State $\M[\hat]{D} = \M[\hat]{A} \M{H}$ %\hfill \Comment{local matrix multiplication}
			\State $\M[\hat]{W} = \textsc{Rank2-NLS-Solve}(\M[\hat]{D},\M{G}_H)$
		\EndWhile
	\EndFunction
	\Ensure{$\M{A} \approx \M{W}\M{H}^\Tra$ with $\M{W}$, $\M{H}$ row-distributed}
\end{algorithmic}
\end{algorithm}

\begin{figure}
\input{fig/split}
\caption{Parallel splitting using Rank-2 NMF}
\label{fig:split}
\end{figure}

\subsubsection{Analysis}

\section{Experimental Results}

\subsection{Experimental Platform}

\GB{Lawton, can you add a description of Summit?  use the recent SC submission as an example}

\subsection{Datasets}

\GB{Lawton, can you add these descriptions?}
\begin{itemize}
	\item synthetic (dense and sparse)
	\item HSI: DC and Aviris
	\item Doc-clustering: Reuters and PubMed
\end{itemize}

\subsection{Performance}

\subsubsection{Rank-2 NMF Strong Scaling}

\GB{Lawton, can you gather this data, Ramki can you visualize it?}
\begin{itemize}
	\item compare 1D distribution scaling with optimal 2D grid scaling
	\item use synthetic short-and-fat dense and squarish sparse
\end{itemize}

\subsubsection{Level Scaling}

\GB{Ramki, can you add a bar plot to show time breakdown across complete levels?  Let's do Aviris and PubMed; just use levels data file}

\subsubsection{Weak Scaling}

\GB{maybe this section is lower priority}
\begin{itemize}
	\item dense and sparse
\end{itemize}

\subsubsection{Strong Scaling}

\begin{itemize}
	\item big dense synthetic
	\item big sparse synthetic
	\item Aviris
	\item PubMed
\end{itemize}

\subsection{Clustering}

\subsubsection{Hyperspectral Imaging}

\GB{Lawton, show some visualization of the tree}

\subsubsection{Document Corpus}

\GB{Lawton, show a visualization of top words in tree}

\section{Conclusion}

\section*{Acknowledgment}

The authors would like to thank Simin Ma for contributions to the algorithmic analysis and John Farrell for his contributions to the implementation of the parallel algorithm.

\bibliography{paper}
\bibliographystyle{plainurl}

\end{document}
