@InProceedings{KP13,
  author    = {Kuang, Da and Park, Haesun},
  title     = {Fast Rank-2 Nonnegative Matrix Factorization for Hierarchical Document Clustering},
  booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  year      = {2013},
  series    = {KDD '13},
  pages     = {739--747},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {2487606},
  doi       = {10.1145/2487575.2487606},
  file      = {:KP13.pdf:PDF},
  isbn      = {978-1-4503-2174-7},
  location  = {Chicago, Illinois, USA},
  numpages  = {9},
}

@Article{GKP15,
  author  = {N. {Gillis} and D. {Kuang} and H. {Park}},
  title   = {Hierarchical Clustering of Hyperspectral Images Using Rank-Two Nonnegative Matrix Factorization},
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  year    = {2015},
  volume  = {53},
  number  = {4},
  pages   = {2066-2078},
  month   = {April},
  doi     = {10.1109/TGRS.2014.2352857},
  file    = {:GKP15.pdf:PDF},
  issn    = {1558-0644},
}

@TechReport{EH+19-TR,
  author      = {Eswar, Srinivas and Hayashi, Koby and Ballard, Grey and Kannan, Ramakrishnan and Matheson, Michael A. and Park, Haesun},
  title       = {{PLANC}: Parallel Low Rank Approximation with Non-negativity Constraints},
  institution = {arXiv},
  year        = {2019},
  number      = {1909.01149},
  abstract    = {We consider the problem of low-rank approximation of massive dense non-negative tensor data, for example to discover latent patterns in video and imaging applications. As the size of data sets grows, single workstations are hitting bottlenecks in both computation time and available memory. We propose a distributed-memory parallel computing solution to handle massive data sets, loading the input data across the memories of multiple nodes and performing efficient and scalable parallel algorithms to compute the low-rank approximation. We present a software package called PLANC (Parallel Low Rank Approximation with Non-negativity Constraints), which implements our solution and allows for extension in terms of data (dense or sparse, matrices or tensors of any order), algorithm (e.g., from multiplicative updating techniques to alternating direction method of multipliers), and architecture (we exploit GPUs to accelerate the computation in this work).We describe our parallel distributions and algorithms, which are careful to avoid unnecessary communication and computation, show how to extend the software to include new algorithms and/or constraints, and report efficiency and scalability results for both synthetic and real-world data sets.},
  file        = {:EH+19-TR.pdf:PDF},
  url         = {https://arxiv.org/abs/1909.01149},
}

