@InProceedings{KP13,
  author    = {Kuang, D. and Park, H.},
  title     = {Fast Rank-2 Nonnegative Matrix Factorization for Hierarchical Document Clustering},
  booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  year      = {2013},
  series    = {KDD '13},
  pages     = {739--747},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {2487606},
  doi       = {10.1145/2487575.2487606},
  file      = {:KP13.pdf:PDF},
  isbn      = {978-1-4503-2174-7},
  location  = {Chicago, Illinois, USA},
  numpages  = {9},
}

@Article{GKP15,
  author  = {N. {Gillis} and D. {Kuang} and H. {Park}},
  title   = {Hierarchical Clustering of Hyperspectral Images Using Rank-Two Nonnegative Matrix Factorization},
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  year    = {2015},
  volume  = {53},
  number  = {4},
  pages   = {2066-2078},
  month   = {April},
  doi     = {10.1109/TGRS.2014.2352857},
  file    = {:GKP15.pdf:PDF},
  issn    = {1558-0644},
}

@TechReport{EH+19-TR,
  author      = {Eswar, S. and Hayashi, K. and Ballard, G. and Kannan, R. and Matheson, M.A. and Park, H.},
  title       = {{PLANC}: Parallel Low Rank Approximation with Non-negativity Constraints},
  institution = {arXiv},
  year        = {2019},
  number      = {1909.01149},
  abstract    = {We consider the problem of low-rank approximation of massive dense non-negative tensor data, for example to discover latent patterns in video and imaging applications. As the size of data sets grows, single workstations are hitting bottlenecks in both computation time and available memory. We propose a distributed-memory parallel computing solution to handle massive data sets, loading the input data across the memories of multiple nodes and performing efficient and scalable parallel algorithms to compute the low-rank approximation. We present a software package called PLANC (Parallel Low Rank Approximation with Non-negativity Constraints), which implements our solution and allows for extension in terms of data (dense or sparse, matrices or tensors of any order), algorithm (e.g., from multiplicative updating techniques to alternating direction method of multipliers), and architecture (we exploit GPUs to accelerate the computation in this work).We describe our parallel distributions and algorithms, which are careful to avoid unnecessary communication and computation, show how to extend the software to include new algorithms and/or constraints, and report efficiency and scalability results for both synthetic and real-world data sets.},
  file        = {:EH+19-TR.pdf:PDF},
  url         = {https://arxiv.org/abs/1909.01149},
}

@Article{BCDH+14,
  Title                    = {Communication lower bounds and optimal algorithms for numerical linear algebra},
  Author                   = {Ballard, G. and Carson, E. and Demmel, J. and Hoemmen, M. and Knight, N. and Schwartz, O.},
  Journal                  = {Acta Numerica},
  Year                     = {2014},

  Month                    = {May},
  Pages                    = {1--155},
  Volume                   = {23},

  Abstract                 = {The traditional metric for the efficiency of a numerical algorithm has been the number of arithmetic operations it performs. Technological trends have long been reducing the time to perform an arithmetic operation, so it is no longer the bottleneck in many algorithms; rather, communication, or moving data, is the bottleneck. This motivates us to seek algorithms that move as little data as possible, either between levels of a memory hierarchy or between parallel processors over a network. In this paper we summarize recent progress in three aspects of this problem. First we describe lower bounds on communication. Some of these generalize known lower bounds for dense classical (O(n^3)) matrix multiplication to all direct methods of linear algebra, to sequential and parallel algorithms, and to dense and sparse matrices. We also present lower bounds for Strassen-like algorithms, and for iterative methods, in particular Krylov subspace methods applied to sparse matrices. Second, we compare these lower bounds to widely used versions of these algorithms, and note that these widely used algorithms usually communicate asymptotically more than is necessary. Third, we identify or invent new algorithms for most linear algebra problems that do attain these lower bounds, and demonstrate large speed-ups in theory and practice.},
  Annote                   = {Invited paper.},
  Doi                      = {10.1017/S0962492914000038},
  File                     = {BCDH+14.pdf:BCDH+14.pdf:PDF},
  ISSN                     = {1474-0508},
  Numpages                 = {155},
}

@Article{CH+07,
  Title                    = {Collective communication: theory, practice, and experience},
  Author                   = {Chan, E. and Heimlich, M. and Purkayastha, A. and van de Geijn, R.},
  Journal                  = {Concurrency and Computation: Practice and Experience},
  Year                     = {2007},
  Number                   = {13},
  Pages                    = {1749--1783},
  Volume                   = {19},

  Doi                      = {10.1002/cpe.1206},
  File                     = {CH+07.pdf:CH+07.pdf:PDF},
  ISSN                     = {1532-0634},
  Keywords                 = {collective communication, distributed-memory architecture, clusters},
  Publisher                = {John Wiley \& Sons, Ltd.},
}

@Article{TRG05,
  Title                    = {Optimization of Collective Communication Operations in {MPICH}},
  Author                   = {Thakur, R. and Rabenseifner, R. and Gropp, W.},
  Journal                  = {International Journal of High Performance Computing Applications},
  Year                     = {2005},
  Number                   = {1},
  Pages                    = {49--66},
  Volume                   = {19},

  Doi                      = {10.1177/1094342005051521},
  File                     = {TRG05.pdf:TRG05.pdf:PDF},
}

@InProceedings{GOS16,
  author    = {Gropp, W. and Olson, L.N. and Samfass, P.},
  title     = {Modeling {MPI} Communication Performance on SMP Nodes: Is It Time to Retire the Ping Pong Test},
  booktitle = {Proceedings of the 23rd European MPI Users' Group Meeting},
  year      = {2016},
  series    = {EuroMPI 2016},
  pages     = {41--50},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  doi       = {10.1145/2966884.2966919},
  file      = {:GOS16.pdf:PDF},
  isbn      = {9781450342346},
  location  = {Edinburgh, United Kingdom},
  numpages  = {10},
}

@Article{DKDP17,
  author    = {Du, R. and Kuang, D. and Drake, B. and Park, H.},
  title     = {{DC-NMF}: nonnegative matrix factorization based on divide-and-conquer for fast clustering and topic modeling},
  journal   = {Journal of Global Optimization},
  year      = {2017},
  volume    = {68},
  number    = {4},
  pages     = {777--798},
  doi       = {10.1007/s10898-017-0515-z},
  file      = {:DKDP17.pdf:PDF},
  publisher = {Springer},
}

@Article{KP11,
  author  = {J. Kim and H. Park},
  title   = {Fast Nonnegative Matrix Factorization: An Active-Set-Like Method and Comparisons},
  journal = {SIAM Journal on Scientific Computing},
  year    = {2011},
  volume  = {33},
  number  = {6},
  pages   = {3261-3281},
  doi     = {10.1137/110821172},
  file    = {KP11.pdf:KP11.pdf:PDF},
  url     = {https://doi.org/10.1137/110821172},
}

@Article{KHP14,
  Title                    = {Algorithms for nonnegative matrix and tensor factorizations: a unified view based on block coordinate descent framework},
  Author                   = {Kim, J. and He, Y. and Park, H.},
  Journal                  = {Journal of Global Optimization},
  Year                     = {2014},
  Number                   = {2},
  Pages                    = {285-319},
  Volume                   = {58},

  Doi                      = {10.1007/s10898-013-0035-4},
  File                     = {:KHP14.pdf:PDF},
  ISSN                     = {0925-5001},
  Language                 = {English},
  Publisher                = {Springer US},
}

@InProceedings{KBP16,
  author      = {Kannan, R. and Ballard, G. and Park, H.},
  title       = {A High-Performance Parallel Algorithm for Nonnegative Matrix Factorization},
  booktitle   = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
  year        = {2016},
  series      = {PPoPP '16},
  pages       = {9:1--9:11},
  address     = {New York, NY, USA},
  month       = {February},
  publisher   = {ACM},
  abstract    = {Non-negative matrix factorization (NMF) is the problem of determining two non-negative low rank factors W and H, for the given input matrix A, such that WH approximates A. NMF is a useful tool for many applications in different domains such as topic modeling in text mining, background separation in video analysis, and community detection in social networks. Despite its popularity in the data mining community, there is a lack of efficient distributed algorithms to solve the problem for big data sets.

We propose a high-performance distributed-memory parallel algorithm that computes the factorization by iteratively solving alternating non-negative least squares (NLS) subproblems for W and H. It maintains the data and factor matrices in memory (distributed across processors), uses MPI for interprocessor communication, and, in the dense case, provably minimizes communication costs (under mild assumptions). As opposed to previous implementations, our algorithm is also flexible: (1) it performs well for both dense and sparse matrices, and (2) it allows the user to choose any one of the multiple algorithms for solving the updates to low rank factors W and H within the alternating iterations. We demonstrate the scalability of our algorithm and compare it with baseline implementations, showing significant performance improvements.},
  acmid       = {2851152},
  articleno   = {9},
  doi         = {10.1145/2851141.2851152},
  file        = {:KBP16.pdf:PDF},
  isbn        = {978-1-4503-4092-2},
  location    = {Barcelona, Spain},
  numpages    = {11},
  techversion = {KBP15-TR},
}

@Article{KBP17,
  author   = {R. Kannan and G. Ballard and H. Park},
  title    = {{MPI-FAUN}: An {MPI}-Based Framework for Alternating-Updating Nonnegative Matrix Factorization},
  journal  = {IEEE Transactions on Knowledge and Data Engineering},
  year     = {2018},
  volume   = {30},
  number   = {3},
  pages    = {544-558},
  month    = {March},
  abstract = {Non-negative matrix factorization (NMF) is the problem of determining two non-negative low rank factors W and H, for the given input matrix A, such that WH approximates A. NMF is a useful tool for many applications in different domains such as topic modeling in text mining, background separation in video analysis, and community detection in social networks. Despite its popularity in the data mining community, there is a lack of efficient parallel algorithms to solve the problem for big data sets. The main contribution of this work is a new, high-performance parallel computational framework for a broad class of NMF algorithms that iteratively solves alternating non-negative least squares (NLS) subproblems for $mathbf{W}$ and $mathbf{H}$ . It maintains the data and factor matrices in memory (distributed across processors), uses MPI for interprocessor communication, and, in the dense case, provably minimizes communication costs (under mild assumptions). The framework is flexible and able to leverage- a variety of NMF and NLS algorithms, including Multiplicative Update, Hierarchical Alternating Least Squares, and Block Principal Pivoting. Our implementation allows us to benchmark and compare different algorithms on massive dense and sparse data matrices of size that spans from few hundreds of millions to billions. We demonstrate the scalability of our algorithm and compare it with baseline implementations, showing significant performance improvements. The code and the datasets used for conducting the experiments are available online.},
  doi      = {10.1109/TKDE.2017.2767592},
  issn     = {1041-4347},
}

@Article{LK+17b,
  author  = {A. P. Liavas and G. Kostoulas and G. Lourakis and K. Huang and N. D. Sidiropoulos},
  title   = {Nesterov-based Alternating Optimization for Nonnegative Tensor Factorization: Algorithm and Parallel Implementation},
  journal = {IEEE Transactions on Signal Processing},
  year    = {2017},
  month   = {Nov},
  doi     = {10.1109/TSP.2017.2777399},
  file    = {:LK+17b.pdf:PDF},
  issn    = {1053-587X},
}

@misc{DC-HYDICE,
  author       = {D. Landgrebe and L. Biehl},
  title        = {MultiSpec - Hyperspectral Images},
  howpublished = {\url{https://engineering.purdue.edu/~biehl/MultiSpec/hyperspectral.html}},
  month        = {February},
  year         = {2020},
}

@misc{SIIM-ISIC,
  key = {SIIM-ISIC},
  title        = {The {ISIC} 2020 Challenge Dataset},
  doi = {10.34970/2020-ds01},
  year         = {2020}
}

@misc{SmallK,
        author       = {B. Drake and S. Lee-Urban and H. Park},
        title        = {SmallK is a {C++}{/}{P}ython high-performance software
                                        library for nonnegative matrix factorization (NMF)
                                        and hierarchical and flat clustering using
                                        the NMF; current version 1.6.2},
        howpublished = {\url{http://smallk.github.io/}},
        month        = {June},
        year         = {2017}
}

@inproceedings{XLG03,
  title={Document clustering based on non-negative matrix factorization},
  author={Xu, W. and Liu, X. and Gong, Y.},
  booktitle={Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={267--273},
  year={2003},
  doi={10.1145/860435.860485}
}

@article{SBPP06,
  title={Document clustering using nonnegative matrix factorization},
  author={Shahnaz, F. and Berry, M.W. and Pauca, V.P. and Plemmons, R.J.},
  journal={Information Processing \& Management},
  volume={42},
  number={2},
  pages={373--386},
  year={2006},
  publisher={Elsevier},
  doi={10.1016/j.ipm.2004.11.005}
}

@inproceedings{DHS05,
  title={On the equivalence of nonnegative matrix factorization and spectral clustering},
  author={Ding, C. and He, X. and Simon, H.D.},
  booktitle={Proceedings of the 2005 SIAM International Conference on Data Mining},
  pages={606--610},
  year={2005},
  organization={SIAM},
  doi={10.1137/1.9781611972757.70},
}

@Article{FKPB15,
  author    = {Fairbanks, J.P. and Kannan, R. and Park, H. and Bader, D.A.},
  title     = {Behavioral clusters in dynamic graphs},
  journal   = {Parallel Computing},
  year      = {2015},
  volume    = {47},
  pages     = {38--50},
  doi       = {10.1016/j.parco.2015.03.002},
  publisher = {Elsevier},
}

@InProceedings{BW09,
  author    = {Battenberg, E. and Wessel, D.},
  title     = {Accelerating Non-Negative Matrix Factorization for Audio Source Separation on Multi-Core and Many-Core Architectures.},
  booktitle = {ISMIR},
  year      = {2009},
  pages     = {501--506},
  url       = {https://archives.ismir.net/ismir2009/paper/000089.pdf},
}

@InProceedings{MESPS20,
  author    = {Gordon E. Moon and J. Austin Ellis and Aravind Sukumaran-Rajam and Srinivasan Parthasarathy and P. Sadayappan},
  title     = {{ALO-NMF}: Accelerated Locality-Optimized Non-negative Matrix Factorization},
  booktitle = {Proceedings of the 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  year      = {2020},
  series    = {KDD '20},
  doi       = {10.1145/3394486.3403227},
  file      = {:MESPS20.pdf:PDF},
}

