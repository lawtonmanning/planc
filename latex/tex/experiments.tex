% !TEX root = ../paper.tex

\section{Experimental Results}
\label{sec:results}

\subsection{Experimental Platform}
\label{sec:summit}

All the experiments in this section were conducted on Summit. Summit is a supercomputer created by IBM for the Oak Ridge National Laboratory. 
There are approximately 4,600 nodes on Summit. Each node contains two IBM POWER9 processors on separate sockets with dual NVLINK bricks to facilitate a transfer rate of 
25 GB/s between the processors. Each node contains 512 GB of DDR4 memory for use by both processors. Each POWER9 processor utilizes 22 IBM SIMD Multi-Cores (SMCs), 
although one of these SMCs on each processor is dedicated to memory transfer and is therefore not available for computation. 
For node scaling experiments, all 42 available SMCs were utilized in each node so that every node computed with 42 separate MPI processes.
Additionally, every node also supports six NVIDIA Volta V100 accelerators but these were unused by our algorithm. 

Our implementation builds on the PLANC open-source library and uses the Armadillo library (version 9.900.1) for all matrix operations. 
%In Armadillo, sparse matrices are stored in Compressed Sparse Column (CSC) format and dense matrices are stored in column-major ordering.
On Summit, we linked this version of Armadillo with OpenBLAS (version 0.3.9) and IBM's Spectrum MPI (version 10.3.1.2-20200121).

\subsection{Datasets}

\paragraph{\emph{Hyperspectral Imaging}}

	We use the Hyperspectral Digital Imagery Collection Experiment (HYDICE) image of the Washington DC Mall. We will refer
	to this dataset as \hyper{}\cite{DC-HYDICE}.
	
	\hyper{} is formatted into a 3-way tensor representing two spatial dimensions of pixels and one dimension of spectral bands. So, a slice along
	the spectral band dimension would be the full \hyper{} image in that spectral band. For hierarchical clustering, these tensors are flattened so that the rows represent the
	$191$ spectral bands and the columns represent the $392960$ pixels.   The data set is approximately 600 MB in size.

	%Larger hyperspectral datasets tend to not grow larger than around 200 spectral bands and have millions or billions of pixels. So, scaling is 
	%limited for this dataset given our data distribution scheme.
	
\paragraph{\emph{Image Classification}}

	The SIIM-ISIC Melanoma classification
	dataset, which we will refer to as \image{}\cite{SIIM-ISIC}, consists of $33126$ RGB training images equally sized at $1024 \times 1024$. Unlike with hyperspectral imaging, the resulting
	matrix used in hierarchical clustering consists of image pixels along the rows and individual images along the columns. So, the resulting sized matrix is
	$3145728 \times 33126$, which is approximately 800 GB in size. 

	Given its size, \image{} requries $10$ Summit nodes to perform hierarchical clustering.
	
\paragraph{\emph{Synthetic Dataset}}

	Our synthetic dataset has the same aspect ratio of \image{} but consists of fewer rows and columns by a factor of $3$. The resulting matrix is $1048576 \times 11042$. 
	We choose the smaller size in order to fit on a single node for scaling experiments.

\subsection{Performance}
\label{sec:perf}
For all hierarchical clustering experiments in this section, the number of tree leaf nodes $k$ was set at $100$, 
the number of NMF iterations was set to $100$,
the power iteration was allowed to stop iterating after convergence,
and only complete levels were considered for analysis purposes for both level and strong scaling plots.

\subsubsection{Single-Node Scaling for DC Dataset}

\GB{add only speedup plot for entire tree (at least all complete levels) from 1 core to 42 cores}

\hyper{} is small compared to the other datasets, so it can easily fit on one compute node.
Also, its small number of 191 rows doesn't allow for parallelizing beyond that number of MPI processes.
So, this dataset was used for a single-node scaling experiment on Summit from 1 to 42 cores.
Because the hierarchical clustering algorithm is memory bandwidth bound, there should be limited speedup on one node due to it being essentially shared memory 
parallelism. Figure \LM{FIXME: DC REF} shows that there is enough speedup for it to be worth parallelizing such a small problem. In this experiment, the
processes were distributed across both sockets so that a minimum amount of shared memory between processes was used.


\begin{itemize}
	\item emphasize we start with a small data set that fits on one node and has only 191 rows
	\item using 1 node (2 sockets), because we are memory bandwidth bound we expect limited speedup but we can save time with parallelism even for this small a data set
\end{itemize}

\subsubsection{Rank-2 NMF Strong Scaling}

We perform strong scaling experiments for a single Rank-2 NMF (\Cref{alg:parrank2nmf}) on the synthetic and \image{} datasets.
The theory (\Cref{eq:r2nmfcost}) suggests that perfect strong scaling is possible as long as the execution time is dominated by local computation.
Both the matrix multiplications and NNLS solves scale linearly with $1/p$ (we expect MatMul to dominate), but the bandwidth cost is independent of $p$ and the latency cost increases slightly with $p$.

\Cref{fig:synrank2speedup,fig:rwrank2speedup} show performance relative to the smallest number of compute nodes required to store data and factor matrices.
The synthetic data is chosen to fit on a single node, while the \image{} data requires 10 compute nodes to store the input matrix as well as temporary and output matrices.
For these data sets, we observe nearly perfect strong scaling, with 47$\times$ speedup on 50 compute nodes (over 1 compute node) for synthetic data and 7.1$\times$ speedup on 80 compute nodes (over 10 compute nodes) for \image{} data.

\begin{figure}
\begin{center}
\includegraphics[height=2in, width=\columnwidth]{plots/synthetic_rank2_speedup.pdf}
\caption{Strong Scaling Speedup for Rank-2 NMF on synthetic data}
\label{fig:synrank2speedup}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[height=2in, width=\columnwidth]{plots/synthetic_rank2_strongscaling.pdf}
\caption{Relative Time Breakdown for Rank-2 NMF on synthetic data}
\label{fig:synrank2strongscaling}
\end{center}
\end{figure}

The relative time breakdowns are presented in \Cref{fig:synrank2strongscaling,fig:rwrank2strongscaling} and explain the strong scaling performance.
Each experiment is normalized to 100\% time, so comparisons cannot be readily made across numbers of compute nodes. 
For both data sets, we see that the time is dominated by MatMul, which is the primary reason for the scalability.
The dominant matrix multiplications are between a large matrix and a matrix with 2 columns, so it is locally memory bandwidth bound, with performance proportional to the size of the large matrix.
In each plot, we also see the relative time of all-gather and reduce-scatter increasing, which is because the local computation is decreasing while the communication cost is slightly increasing with $p$.
This pattern will continue as $p$ increases, which will eventually limit scalability, but for these data sets the MatMul takes around 80\% of the time at over 2000 cores.

\begin{figure}
\begin{center}
\includegraphics[height=2in, width=\columnwidth]{plots/realworld_rank2_speedup.pdf}
\caption{Strong Scaling Speedup for Rank-2 NMF on \image{} data}
\label{fig:rwrank2speedup}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[height=2in, width=\columnwidth]{plots/realworld_rank2_strongscaling.pdf}
\caption{Relative Time Breakdown for Rank-2 NMF on \image{} data}
\label{fig:rwrank2strongscaling}
\end{center}
\end{figure}


\subsubsection{Hierarchical Clustering Strong Scaling}
\label{sec:hiernmf2scaling}

\GB{Do these breakdowns include the score function?  actually this may explain why all-reduce costs more, it also has $O(n)$ data for score...}

\GB{4 plots: 2 data sets with speedup/breakdown, note that we include time for complete levels, ignoring nodes in incomplete levels for easier interpretation}


From equation \ref{eq:treecost}, we expect to see perfect strong scaling in a computationally bound hierarchical clustering problem with fixed frontier node $k=100$ count.
The problem should become latency bound with its $\log p$ cost as we scale. 

Figure \ref{fig:synhierstrongscaling} shows the limits of scaling as the hierarchical cost is becoming saturated with communication.
The AllReduce segment would consistently grow if the problem was becoming latency bound, which is not the case.
So, the problem is becoming bandwidth bound as seen through the ReduceScatter and AllGather segments. 
However, computation still makes up more than 60\% of the total cost.

With the much larger \image{} dataset, it's possible to scale much further as seen in Figure \ref{fig:rwhierspeedup} without being overwhelmed by the communication cost.
From Figure \ref{fig:fig:rwhierstrongscaling}, the communication cost doesn't make up more than 20\% of the total cost even scaling to 80 compute nodes. 

Note that in order to compare hierarchical clusters between differing compute nodes, we limited the output to include only complete levels in the tree.

\begin{itemize}
	\item theory (\Cref{eq:treecost}) says...
	\item Fig 7 shows limit of strong scaling for synthetic data (time increases at 50 nodes compared to 40), Fig 8 gives breakdown and explains
	\begin{itemize}
		\item drastic performance dropoff may be artifact of all-gather, which should take about the same time as reduce-scatter, but speedup from 30 to 40 nodes is also slight
		\item by 50 nodes, about 50\% of time in communication; still mostly dominated by bandwidth cost because ag and rs dominate ar
		\item note the difference between the scaling of single rank-2 and entire tree: poor scaling must be due to lower levels, which we explore in the next section
	\end{itemize}
	\item Fig 9 shows reasonable scaling for entire tree -- speedup drops from 7x to 6x over 80 nodes; Fig 10 shows that MM is still 70\% of run time on 80 nodes, which allows for near-linear scaling
\end{itemize}

\begin{figure}
\begin{center}
\includegraphics[height=2in, width=\columnwidth]{plots/synthetic_hierarchical_speedup.pdf}
\caption{Strong Scaling Speedup for Hierarchical Clustering. The total time taken for complete Hierarchical Clustering \cref{alg:hiernmf2}}
\label{fig:synhierspeedup}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[height=2in, width=\columnwidth]{plots/synthetic_hier_strongscaling.pdf}
\caption{Relative Time Breakdown for Hierarchical Clustering. The plot uses the total time taken for complete Hierarchical Clustering \cref{alg:hiernmf2}}
\label{fig:synhierstrongscaling}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[height=2in, width=\columnwidth]{plots/realworld_hierarchical_speedup.pdf}
\caption{Strong Scaling Speedup for Hierarchical Clustering on \image{} Data. The total time taken for complete Hierarchical Clustering \cref{alg:hiernmf2}}
\label{fig:rwhierspeedup}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[height=2in, width=\columnwidth]{plots/realworld_hier_strongscaling.pdf}
\caption{Relative Time Breakdown for Hierarchical Clustering on \image{}. The plot uses the total time taken for complete Hierarchical Clustering \cref{alg:hiernmf2}}
\label{fig:rwhierstrongscaling}
\end{center}
\end{figure}

\subsubsection{Level Scaling}

\GB{3 plots: for synthetic level breakdown for 1 node and for 40 nodes; for realworld just for 80 nodes}

To show the level scaling in a particular tree, we only considered complete levels as the analysis from Equation \ref{eq:levelcost} only considers
complete levels. From Equation \ref{eq:levelcost}, the high order computational term (represented by MatMul) is constant per level, the lower order 
computational term (represented by NNLS) grows like $O(2^\ell)$, and the latency cost grows similarly like $O(2^\ell)$. 

From Figure \ref{fig:seqlevelbreakdown}, the MatMul cost decreases slightly per level, which may be explained by BLAS optimizations occurring in the
local matrix multiply, especially as each node's subproblem becomes smaller and smaller. The NNLS grows exponentially, which is exactly in line with theory,
and none of the levels seem to become bandwidth or latency bound for this case.

As we increase the nodes to 40 in the same sequential case, the story completely changes. Figure \ref{fig:parallellevelbreakdown} shows the MatMul cost is almost precisely the same, which is line with the theory.
The NNLS cost grows exponentially as before and becomes dominating at the lower levels. The latency is also appearing as the AllReduce cost, which is latency bound,
starts to grow exponentially from level 3 onwards. 

In Figure \ref{fig:rwparallellevelbreakdown}, note that this levels experiment is from the image dataset, which is considerably larger than the synthetic case but is of the same size.
Although all is relatively the same as in Figure \ref{fig:parallellevelbreakdown}, matrix multiply still dominates at the lower levels and the problem is computationally bound
at each level, which is indiciative of the strong speedup as seen in the previous section.

\begin{itemize}
	\item theory says per-level cost is \Cref{eq:levelcost}
\end{itemize}

\begin{figure}
\begin{center}
\includegraphics[height=2in, width=\columnwidth]{plots/synthetic_sequential_level_breakdown.pdf}
\caption{Sequential Breakdown plot for levels 0 to 5. y-axis is absolute wall clock time in seconds}
\label{fig:seqlevelbreakdown}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[height=2in, width=\columnwidth]{plots/synthetic_parallel_level_breakdown.pdf}
\caption{Parallel Breakdown plot for levels 0 to 5 on 40 nodes. y-axis is absolute wall clock time in seconds}
\label{fig:parallellevelbreakdown}
\end{center}
\end{figure}


\begin{figure}
\begin{center}
\includegraphics[height=2in, width=\columnwidth]{plots/realworld_parallel_level_breakdown.pdf}
\caption{Parallel Breakdown plot for levels 0 to 5 on 80 nodes. y-axis is absolute wall clock time in seconds}
\label{fig:rwparallellevelbreakdown}
\end{center}
\end{figure}


